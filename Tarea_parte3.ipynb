{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EOazdPCCCVfi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cambiamos al directorio del drive y le damos autorizacion\n",
        "\n",
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMBEbl4yEkh0",
        "outputId": "c108e824-964c-4fd6-e8de-ba380d08e0a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Renombramos My Drive a mydrive para evitar problemas con el espacio \" \"\n",
        "\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_V_blJzEklg",
        "outputId": "e6d8de9a-331f-495e-9178-4a4e3f34b91c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln: failed to create symbolic link '/mydrive/My Drive': File exists\n",
            " 20180914_Waiver_Juan_Pablo.pdf\n",
            "'2020_Book_MachineLearningForMedicalImage (1).pdf'\n",
            " 2-Aurélien-Géron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O’Reilly-Media-2019.pdf\n",
            " 2dtodepth\n",
            "'Arc Welding Reliability Model-rev1-presentacion jueves-Mark up-rev2.pptx'\n",
            " Auto_econder.ipynb\n",
            " bin\n",
            " Bombas.rar\n",
            " catboost_info\n",
            " CCWJ\n",
            " CCWJ_Meeting_Minutes_499.gdoc\n",
            " CCWJ_Meeting_Minutes_499.pdf\n",
            " Data_navy\n",
            " Example\n",
            " Form100_AppendixD_Juan_Pablo.pdf\n",
            " Foto_swiss_contact.jpg\n",
            " logs.log\n",
            " Medical_care\n",
            " Memoria_CCWJ_jprc_rev00000.pdf\n",
            " Memoria_CCWJ_jprc_rev0.pdf\n",
            " Memoria_CCWJ_rev3.pdf\n",
            " Memoria_CCWJ_rev4.pdf\n",
            " Memoria_CCWJ_rev5.pdf\n",
            "'My Drive'\n",
            " Pycaret\n",
            " Pytorch\n",
            "'Returning_to_Campus_-_Record_of_Completion (3).pdf'\n",
            " Returning_to_Campus_-_Record_of_Completion.pdf\n",
            " sample_img\n",
            " saved_model.pkl\n",
            "'Surface topography semantic segmentation'\n",
            " visitor-waiver-form.gdoc\n",
            " visitor-waiver-form_Juan_Pablo_Romero_Campos.pdf\n",
            " visitor-waiver-form.pdf\n",
            " Welding-Defect-Analysis.png\n",
            "'Work permit (1).jpeg'\n",
            "'Work permit.jpeg'\n",
            " Yolo2_d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mydrive/Data_navy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEv87jkbEkn7",
        "outputId": "2ce6dcab-43fd-4133-f0f9-7283bd3723f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Data_navy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('file_name.csv')"
      ],
      "metadata": {
        "id": "pUZ1fdKeFDcN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(0, inplace=True)\n",
        "df.isna().sum()\n",
        "power = df['Mean_global_active_power'].values.astype('float32')"
      ],
      "metadata": {
        "id": "TtPh1uOEGEdt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rlb8pY2tVVi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-2*look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back)]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back: i+2*look_back])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ],
      "metadata": {
        "id": "xdBvUvwkJLim"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = create_dataset(power, look_back=365)"
      ],
      "metadata": {
        "id": "WugArmLVbq0G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM-a0Q3ewoyE",
        "outputId": "7559cf96-7b72-4095-f7c8-2cbf01e2713a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(711, 365)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "Y = scaler.transform(Y)"
      ],
      "metadata": {
        "id": "ZhmiFpU1yMX4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape(711, 365, 1)"
      ],
      "metadata": {
        "id": "fPTr1k1Tbq88"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Y.reshape(711, 365, 1)"
      ],
      "metadata": {
        "id": "7bf0EqMJb1m1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se separan los datos de entrenamiento, validacion y testeo\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle=False)"
      ],
      "metadata": {
        "id": "zskKenCJUWik"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edQ4su6lwt5n",
        "outputId": "2bed22d8-a882-4201-d580-f8c6b5e4f9f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(476, 365, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se separan los datos de entrenamiento, validacion y testeo\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, shuffle=False)"
      ],
      "metadata": {
        "id": "5u-D2HT3UWm7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "#x_train = scaler.fit_transform(x_train).reshape(-1,1)\n",
        "#y_train = scaler.transform(y_train).reshape(-1,1)\n",
        "\n",
        "#x_val = scaler.transform(x_val).reshape(-1,1)\n",
        "#y_val = scaler.transform(y_val).reshape(-1,1)\n",
        "#x_test = scaler.transform(x_test).reshape(-1,1)\n",
        "#y_test = scaler.transform(y_test).reshape(-1,1)\n"
      ],
      "metadata": {
        "id": "YqnVpZ0utbhT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import time\n",
        "# metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "3wWRKqLzZlO4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gRoSMYUcYgD",
        "outputId": "5cfc9308-ded6-4feb-88b2-d9828be2e75d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(365, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GR8y-Gxxcyp",
        "outputId": "8c96fc4c-f8f9-45b6-a7e6-5fd6d543b6f9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357, 365, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Modelo Simple LSTM\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.LSTM(128, activation='relu', input_shape =x_train.shape[1:], return_sequences=True))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.LSTM(64,  activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "### NN densa\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, input_shape=(8,)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(32, activation = 'relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(16, activation = 'relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "            loss='mean_squared_error',\n",
        "            metrics=['mean_squared_error'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwlMTEu7FDhJ",
        "outputId": "d13e7cf7-b19d-4654-98ac-4760e7b38ead"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 365, 128)          66560     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 365, 128)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               8320      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128)              512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 135,681\n",
            "Trainable params: 135,425\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, validation_split = 0.2, epochs=300, batch_size = 512,  verbose=1).history\n",
        "loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "print('Final training time [min]: {:.2f}'.format((time.time()-start_time)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SyY_3XneNuv",
        "outputId": "f2570d4b-dc7b-4ce5-96f8-626358743540"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 10s 10s/step - loss: 1.5390 - mean_squared_error: 1.5390 - val_loss: 0.2007 - val_mean_squared_error: 0.2007\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7797 - mean_squared_error: 0.7797 - val_loss: 0.1979 - val_mean_squared_error: 0.1979\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6032 - mean_squared_error: 0.6032 - val_loss: 0.1954 - val_mean_squared_error: 0.1954\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4302 - mean_squared_error: 0.4302 - val_loss: 0.1921 - val_mean_squared_error: 0.1921\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3598 - mean_squared_error: 0.3598 - val_loss: 0.1882 - val_mean_squared_error: 0.1882\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2746 - mean_squared_error: 0.2746 - val_loss: 0.1847 - val_mean_squared_error: 0.1847\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2703 - mean_squared_error: 0.2703 - val_loss: 0.1812 - val_mean_squared_error: 0.1812\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2528 - mean_squared_error: 0.2528 - val_loss: 0.1776 - val_mean_squared_error: 0.1776\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2218 - mean_squared_error: 0.2218 - val_loss: 0.1739 - val_mean_squared_error: 0.1739\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1948 - mean_squared_error: 0.1948 - val_loss: 0.1699 - val_mean_squared_error: 0.1699\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1873 - mean_squared_error: 0.1873 - val_loss: 0.1658 - val_mean_squared_error: 0.1658\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1747 - mean_squared_error: 0.1747 - val_loss: 0.1617 - val_mean_squared_error: 0.1617\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1786 - mean_squared_error: 0.1786 - val_loss: 0.1574 - val_mean_squared_error: 0.1574\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1681 - mean_squared_error: 0.1681 - val_loss: 0.1528 - val_mean_squared_error: 0.1528\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1631 - mean_squared_error: 0.1631 - val_loss: 0.1479 - val_mean_squared_error: 0.1479\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1612 - mean_squared_error: 0.1612 - val_loss: 0.1429 - val_mean_squared_error: 0.1429\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1561 - mean_squared_error: 0.1561 - val_loss: 0.1377 - val_mean_squared_error: 0.1377\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1480 - mean_squared_error: 0.1480 - val_loss: 0.1324 - val_mean_squared_error: 0.1324\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1397 - mean_squared_error: 0.1397 - val_loss: 0.1272 - val_mean_squared_error: 0.1272\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1392 - mean_squared_error: 0.1392 - val_loss: 0.1220 - val_mean_squared_error: 0.1220\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1381 - mean_squared_error: 0.1381 - val_loss: 0.1167 - val_mean_squared_error: 0.1167\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1265 - mean_squared_error: 0.1265 - val_loss: 0.1114 - val_mean_squared_error: 0.1114\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1329 - mean_squared_error: 0.1329 - val_loss: 0.1060 - val_mean_squared_error: 0.1060\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1265 - mean_squared_error: 0.1265 - val_loss: 0.1006 - val_mean_squared_error: 0.1006\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1188 - mean_squared_error: 0.1188 - val_loss: 0.0952 - val_mean_squared_error: 0.0952\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1193 - mean_squared_error: 0.1193 - val_loss: 0.0899 - val_mean_squared_error: 0.0899\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1126 - mean_squared_error: 0.1126 - val_loss: 0.0847 - val_mean_squared_error: 0.0847\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1129 - mean_squared_error: 0.1129 - val_loss: 0.0797 - val_mean_squared_error: 0.0797\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1043 - mean_squared_error: 0.1043 - val_loss: 0.0746 - val_mean_squared_error: 0.0746\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1061 - mean_squared_error: 0.1061 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1082 - mean_squared_error: 0.1082 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1002 - mean_squared_error: 0.1002 - val_loss: 0.0607 - val_mean_squared_error: 0.0607\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1024 - mean_squared_error: 0.1024 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1022 - mean_squared_error: 0.1022 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0959 - mean_squared_error: 0.0959 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0965 - mean_squared_error: 0.0965 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0926 - mean_squared_error: 0.0926 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0921 - mean_squared_error: 0.0921 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0919 - mean_squared_error: 0.0919 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0916 - mean_squared_error: 0.0916 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0891 - mean_squared_error: 0.0891 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0871 - mean_squared_error: 0.0871 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0848 - mean_squared_error: 0.0848 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0866 - mean_squared_error: 0.0866 - val_loss: 0.0336 - val_mean_squared_error: 0.0336\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0851 - mean_squared_error: 0.0851 - val_loss: 0.0332 - val_mean_squared_error: 0.0332\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0821 - mean_squared_error: 0.0821 - val_loss: 0.0332 - val_mean_squared_error: 0.0332\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0333 - val_mean_squared_error: 0.0333\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0846 - mean_squared_error: 0.0846 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0776 - mean_squared_error: 0.0776 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0766 - mean_squared_error: 0.0766 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0755 - mean_squared_error: 0.0755 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0782 - mean_squared_error: 0.0782 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0716 - mean_squared_error: 0.0716 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0805 - mean_squared_error: 0.0805 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0754 - mean_squared_error: 0.0754 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0737 - mean_squared_error: 0.0737 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0738 - mean_squared_error: 0.0738 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0667 - mean_squared_error: 0.0667 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0707 - mean_squared_error: 0.0707 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0678 - mean_squared_error: 0.0678 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0698 - mean_squared_error: 0.0698 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0673 - mean_squared_error: 0.0673 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0673 - mean_squared_error: 0.0673 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0699 - mean_squared_error: 0.0699 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0664 - mean_squared_error: 0.0664 - val_loss: 0.0340 - val_mean_squared_error: 0.0340\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0669 - mean_squared_error: 0.0669 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0660 - mean_squared_error: 0.0660 - val_loss: 0.0329 - val_mean_squared_error: 0.0329\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0688 - mean_squared_error: 0.0688 - val_loss: 0.0324 - val_mean_squared_error: 0.0324\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0654 - mean_squared_error: 0.0654 - val_loss: 0.0319 - val_mean_squared_error: 0.0319\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0625 - mean_squared_error: 0.0625 - val_loss: 0.0314 - val_mean_squared_error: 0.0314\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0710 - mean_squared_error: 0.0710 - val_loss: 0.0309 - val_mean_squared_error: 0.0309\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0614 - mean_squared_error: 0.0614 - val_loss: 0.0305 - val_mean_squared_error: 0.0305\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0667 - mean_squared_error: 0.0667 - val_loss: 0.0302 - val_mean_squared_error: 0.0302\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0657 - mean_squared_error: 0.0657 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0660 - mean_squared_error: 0.0660 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0644 - mean_squared_error: 0.0644 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0664 - mean_squared_error: 0.0664 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0646 - mean_squared_error: 0.0646 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0619 - mean_squared_error: 0.0619 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0630 - mean_squared_error: 0.0630 - val_loss: 0.0296 - val_mean_squared_error: 0.0296\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0594 - mean_squared_error: 0.0594 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0614 - mean_squared_error: 0.0614 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0615 - mean_squared_error: 0.0615 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0584 - mean_squared_error: 0.0584 - val_loss: 0.0300 - val_mean_squared_error: 0.0300\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0621 - mean_squared_error: 0.0621 - val_loss: 0.0300 - val_mean_squared_error: 0.0300\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0609 - mean_squared_error: 0.0609 - val_loss: 0.0300 - val_mean_squared_error: 0.0300\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0570 - mean_squared_error: 0.0570 - val_loss: 0.0300 - val_mean_squared_error: 0.0300\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0595 - mean_squared_error: 0.0595 - val_loss: 0.0299 - val_mean_squared_error: 0.0299\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0618 - mean_squared_error: 0.0618 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0564 - mean_squared_error: 0.0564 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0596 - mean_squared_error: 0.0596 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0576 - mean_squared_error: 0.0576 - val_loss: 0.0293 - val_mean_squared_error: 0.0293\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0606 - mean_squared_error: 0.0606 - val_loss: 0.0292 - val_mean_squared_error: 0.0292\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0604 - mean_squared_error: 0.0604 - val_loss: 0.0290 - val_mean_squared_error: 0.0290\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0604 - mean_squared_error: 0.0604 - val_loss: 0.0289 - val_mean_squared_error: 0.0289\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.0289 - val_mean_squared_error: 0.0289\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0589 - mean_squared_error: 0.0589 - val_loss: 0.0288 - val_mean_squared_error: 0.0288\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0594 - mean_squared_error: 0.0594 - val_loss: 0.0288 - val_mean_squared_error: 0.0288\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0591 - mean_squared_error: 0.0591 - val_loss: 0.0286 - val_mean_squared_error: 0.0286\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0285 - val_mean_squared_error: 0.0285\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0580 - mean_squared_error: 0.0580 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0573 - mean_squared_error: 0.0573 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0569 - mean_squared_error: 0.0569 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0576 - mean_squared_error: 0.0576 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0552 - mean_squared_error: 0.0552 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0552 - mean_squared_error: 0.0552 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0573 - mean_squared_error: 0.0573 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0583 - mean_squared_error: 0.0583 - val_loss: 0.0284 - val_mean_squared_error: 0.0284\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0597 - mean_squared_error: 0.0597 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0572 - mean_squared_error: 0.0572 - val_loss: 0.0283 - val_mean_squared_error: 0.0283\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0597 - mean_squared_error: 0.0597 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0558 - mean_squared_error: 0.0558 - val_loss: 0.0281 - val_mean_squared_error: 0.0281\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0553 - mean_squared_error: 0.0553 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0586 - mean_squared_error: 0.0586 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0564 - mean_squared_error: 0.0564 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0566 - mean_squared_error: 0.0566 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0559 - mean_squared_error: 0.0559 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0571 - mean_squared_error: 0.0571 - val_loss: 0.0278 - val_mean_squared_error: 0.0278\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0532 - mean_squared_error: 0.0532 - val_loss: 0.0278 - val_mean_squared_error: 0.0278\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0567 - mean_squared_error: 0.0567 - val_loss: 0.0278 - val_mean_squared_error: 0.0278\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0555 - mean_squared_error: 0.0555 - val_loss: 0.0277 - val_mean_squared_error: 0.0277\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0564 - mean_squared_error: 0.0564 - val_loss: 0.0276 - val_mean_squared_error: 0.0276\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0275 - val_mean_squared_error: 0.0275\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0525 - mean_squared_error: 0.0525 - val_loss: 0.0275 - val_mean_squared_error: 0.0275\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0561 - mean_squared_error: 0.0561 - val_loss: 0.0274 - val_mean_squared_error: 0.0274\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0553 - mean_squared_error: 0.0553 - val_loss: 0.0273 - val_mean_squared_error: 0.0273\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0545 - mean_squared_error: 0.0545 - val_loss: 0.0272 - val_mean_squared_error: 0.0272\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0270 - val_mean_squared_error: 0.0270\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0536 - mean_squared_error: 0.0536 - val_loss: 0.0269 - val_mean_squared_error: 0.0269\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0535 - mean_squared_error: 0.0535 - val_loss: 0.0268 - val_mean_squared_error: 0.0268\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0565 - mean_squared_error: 0.0565 - val_loss: 0.0267 - val_mean_squared_error: 0.0267\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0546 - mean_squared_error: 0.0546 - val_loss: 0.0266 - val_mean_squared_error: 0.0266\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0532 - mean_squared_error: 0.0532 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0518 - mean_squared_error: 0.0518 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0264 - val_mean_squared_error: 0.0264\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0507 - mean_squared_error: 0.0507 - val_loss: 0.0263 - val_mean_squared_error: 0.0263\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0507 - mean_squared_error: 0.0507 - val_loss: 0.0263 - val_mean_squared_error: 0.0263\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0536 - mean_squared_error: 0.0536 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0559 - mean_squared_error: 0.0559 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0546 - mean_squared_error: 0.0546 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0514 - mean_squared_error: 0.0514 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0519 - mean_squared_error: 0.0519 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0519 - mean_squared_error: 0.0519 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0513 - mean_squared_error: 0.0513 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0528 - mean_squared_error: 0.0528 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0533 - mean_squared_error: 0.0533 - val_loss: 0.0261 - val_mean_squared_error: 0.0261\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0533 - mean_squared_error: 0.0533 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0500 - mean_squared_error: 0.0500 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0550 - mean_squared_error: 0.0550 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0519 - mean_squared_error: 0.0519 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0531 - mean_squared_error: 0.0531 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0525 - mean_squared_error: 0.0525 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0508 - mean_squared_error: 0.0508 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0511 - mean_squared_error: 0.0511 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0514 - mean_squared_error: 0.0514 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0516 - mean_squared_error: 0.0516 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0496 - mean_squared_error: 0.0496 - val_loss: 0.0260 - val_mean_squared_error: 0.0260\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0517 - mean_squared_error: 0.0517 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0530 - mean_squared_error: 0.0530 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0513 - mean_squared_error: 0.0513 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0509 - mean_squared_error: 0.0509 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0509 - mean_squared_error: 0.0509 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0511 - mean_squared_error: 0.0511 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0505 - mean_squared_error: 0.0505 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0512 - mean_squared_error: 0.0512 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0535 - mean_squared_error: 0.0535 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0499 - mean_squared_error: 0.0499 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0523 - mean_squared_error: 0.0523 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0498 - mean_squared_error: 0.0498 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0512 - mean_squared_error: 0.0512 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0511 - mean_squared_error: 0.0511 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0543 - mean_squared_error: 0.0543 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0507 - mean_squared_error: 0.0507 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0496 - mean_squared_error: 0.0496 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0533 - mean_squared_error: 0.0533 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0519 - mean_squared_error: 0.0519 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0520 - mean_squared_error: 0.0520 - val_loss: 0.0258 - val_mean_squared_error: 0.0258\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0523 - mean_squared_error: 0.0523 - val_loss: 0.0256 - val_mean_squared_error: 0.0256\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0516 - mean_squared_error: 0.0516 - val_loss: 0.0255 - val_mean_squared_error: 0.0255\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0496 - mean_squared_error: 0.0496 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0252 - val_mean_squared_error: 0.0252\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0476 - mean_squared_error: 0.0476 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0488 - mean_squared_error: 0.0488 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0493 - mean_squared_error: 0.0493 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0505 - mean_squared_error: 0.0505 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0502 - mean_squared_error: 0.0502 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0519 - mean_squared_error: 0.0519 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0506 - mean_squared_error: 0.0506 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0485 - mean_squared_error: 0.0485 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0499 - mean_squared_error: 0.0499 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0500 - mean_squared_error: 0.0500 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0488 - mean_squared_error: 0.0488 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0482 - mean_squared_error: 0.0482 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0483 - mean_squared_error: 0.0483 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0498 - mean_squared_error: 0.0498 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0502 - mean_squared_error: 0.0502 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0492 - mean_squared_error: 0.0492 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0472 - mean_squared_error: 0.0472 - val_loss: 0.0250 - val_mean_squared_error: 0.0250\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0485 - mean_squared_error: 0.0485 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0472 - mean_squared_error: 0.0472 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0485 - mean_squared_error: 0.0485 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0490 - mean_squared_error: 0.0490 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0248 - val_mean_squared_error: 0.0248\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0493 - mean_squared_error: 0.0493 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0481 - mean_squared_error: 0.0481 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0492 - mean_squared_error: 0.0492 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0498 - mean_squared_error: 0.0498 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0472 - mean_squared_error: 0.0472 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0472 - mean_squared_error: 0.0472 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0472 - mean_squared_error: 0.0472 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0490 - mean_squared_error: 0.0490 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0483 - mean_squared_error: 0.0483 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0481 - mean_squared_error: 0.0481 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0483 - mean_squared_error: 0.0483 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0470 - mean_squared_error: 0.0470 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0246 - val_mean_squared_error: 0.0246\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0244 - val_mean_squared_error: 0.0244\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0242 - val_mean_squared_error: 0.0242\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0241 - val_mean_squared_error: 0.0241\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0240 - val_mean_squared_error: 0.0240\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0240 - val_mean_squared_error: 0.0240\n",
            "Final training time [min]: 6.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lJT2TUceN5r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OimkZRCSUylT",
        "outputId": "71a8a94e-2857-4d1c-caf3-d0049eb8f098"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(365, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_2011 = model.predict([X[-1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOT_jEZpvBK",
        "outputId": "cfd61d8f-6415-4221-cff2-25984b7d6d31"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 365, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 365, 1), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 1).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_2011"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAq_OYzPp7qB",
        "outputId": "a2067b87-3f55-4781-c8f8-536265b2308a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288],\n",
              "       [0.38432288]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Y_2011)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "D2s2DkiEsu9w",
        "outputId": "cbc7cb1d-6bea-4e7d-adf7-7dd922121c72"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6fd3701a10>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CD8h1hJAsvA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7URFEuIvsvEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}